% !TeX program = xelatex
% !TeX encoding = UTF-8
\documentclass{MathorCupmodeling}
\usepackage{mwe,color,float}
\usepackage[linesnumbered,ruled]{algorithm2e}
\usepackage{setspace}
\usepackage{pdfpages}
\usepackage{graphicx}
\extrafloats{100}
\bianhao{MCB2301959}
\tihao{A}
\timu{\textbf{基于卷积神经网络CNN图像特征提取的SVM坑洼道路检测与识别}}
\keyword{计算机视觉；卷积神经网络；支持向量机；特征提取；模型融合；可视化多维评估}
\begin{document}
	\begin{abstract}
		啥也不说了，先占个坑，等我有空再写。
	\end{abstract}

	\pagestyle{empty}
	\tableofcontents
	\newpage
	\pagestyle{fancy}

	\setcounter{page}{1}
	\section{问题的提出}
	\subsection{问题背景}
	坑洼道路的检测与识别工作是推动自动无人驾驶、地质勘探、航天科学及自然灾害等领域研究和应用的不可或缺的计算机视觉任务。然而传统的分类算法往往因坑洼图像的复杂及多变性，不能取得较好的效果。近年来，深度学习邻域技术的发展为坑洼道路的检测、为当前存在的亟待解决的问题提供了新的思想。

	在实际工程应用中，非结构化的坑洼道路检测将遇到复杂多样的环境，例如，道路周围环境存在差异、光照条件变化等，这无疑增加了基于视觉检测道路的难度\textcolor{blue}{\cite{曹江华}}。道路坑洼的检测是实现智能车辆导航的基础，只有快速准确地检测出道路的可行驶区域才能够真正实现自动驾驶。

	深度学习拥有很强的特征提取与表示能力，可从图像中提取重要特征。于坑洼道路检测和识别而言，其可识别坑洼的轮廓、纹理、形态等特征，并将上述特征转换为更易分类的表现形式。在深度学习的基础上，迁移学习、知识蒸馏等技术可进一步提升分类性能以更加准确对新出现的道路图像自动识别。
	\subsection{问题要求}
	\begin{itemize}
		\item {\heiti 初赛：}
		\begin{itemize}
			\item \textbf{问题一：}基于图像文件，提取图像特征，以“正常”和“坑洼”为特征建立一个识别率高、速度快且分类准确的模型。
			\item \textbf{问题二：}对问题一中所建立的模型进行训练，并对其进行多维度的评估分析。
			\item \textbf{问题三：}利用训练模型识别测试集中的坑洼图像，并展分析结果的合理性。
		\end{itemize}
		\item {\heiti 复赛：}
		\begin{itemize}
			\item \textbf{问题一}：建立可以更精准地识别坑洼边缘的模型，以满足道路避障、道路修复等工作的需求。
			\item \textbf{问题二}：依据识别的坑洼边缘，对其面积进行估算，计算各坑洼占其图像面积的百分比。
		\end{itemize}
	\end{itemize}
	\section{问题的分析}
	\subsection{问题的整体分析}
	该题是一个基于计算机视觉的坑洼道路的图像数据分析、检测类问题。

	\textbf{从分析目的看}，本题需要分析道路坑洼图像并提取其边缘、面积等特性，建立一个识别率高、速度快、准确的普适性模型。因此本题需要完成以下两方面任务：{\heiti 其一}，分析、研究道路坑洼的边缘特性，检测其边缘，为后续模型的建立提供支撑；同时，用以完成道路避障、道路修复等工作。{\heiti 其二}，根据上述的提取的边缘特性，估算图像中坑洼的像素占比。

	\textbf{从图像数据特征看}，本题的道路图像具有数量多，环境等情况复杂，以及坑洼形态多样等特征，极大地增加的模型学习的难度。因此优先考虑深度学习的方法，利用其强大的特征提取与表示能力，对道路图像进行特征提取。

	\textbf{从模型的选择看}，本题图像数量多，特征复杂，且需要对道路中的坑洼边缘进行检测，同时估算其像素占比。因此，我们考虑图像增强技术，建立基于Ultralytics平台的YOLO（You Only Look Once, YOLO）算法，对道路的坑洼特性进行学习。

	\textbf{从编程软件的选择看}，本题为图像大数据分析类，需要对大量的图片数据进行分析，并依据设问建立合适的模型，对坑洼道路的边缘进行提取，估算其像素占比，因此我们选择使用基于Pycharm内核的Python Jupyter对问题进行求解，其交互式的编程范式及轻量化，方便且高效。
	\subsection{初赛总结}
		{\heiti 针对问题一}，需要提取给定图像的特征，建立一个用于识别其为正常或是坑洼道路的识别率高、速度快、分类准确的模型。首先，对图像文件\textbf{预处理}，包括\textbf{人为再分类}，对标记错误的进行\textbf{修正}，或\textbf{剔除异常图像}；再将两类图像置于各子文件中。其次，从\textbf{机理层面}对正常与坑洼道路进行比较分析，包括\textbf{原图、热力图、RGB与灰度直方图的对比、边缘与轮廓检测、阈值分割}，从而有效地提取出\textbf{图像特征}。之后，利用\textbf{opencv-python}读取图像，并加入\textbf{try-except异常处理}语块，\textbf{防止破损文件污染模型}；此外，计算出正常与坑洼道路的个数比值为$\boldsymbol{6.32}$，即\textbf{数据集类别不平衡}，因此采用\textbf{基于抽样的方法平衡化}；之后，以$\boldsymbol{8:2}$的比例划分数据集为\textbf{训练集与测试集}；考虑到现有的图像未能较全面地展现特征，因此\textbf{对训练集采用高斯、双边滤波，旋转处理进行图像增强}。随后，建立支持向量机、卷积神经网络的单一模型，但由于\textbf{CNN中softmax函数的局限性}，因而，\textbf{利用卷积神经网络对图像信息的特征进行提取}，依据上述分析，设定其输入层、卷积层、激活层、池化层、全连接层参数；在此基础上，建立\textbf{支持向量机分类模型}，对道路类别进行分类。

		{\heiti 针对问题二}，需要对问题一的模型进行训练，并进行多维度的评估分析，保证模型的准确性、快速性及普适性。因此，对问题一所建立的模型进行训练与测试，并从\textbf{模型耗时、准确率、分类报告、五折交叉验证，混淆矩阵、ROC/AUC曲线}进行多维度评价。模型\textbf{准确率在$\boldsymbol{95\,\%}$的置信水平下达到$\boldsymbol{\left(90\pm 8\right)\,\%}$}；其\textbf{精确率、召回率、F1分数值在各维度下均于$\boldsymbol{88\,\%}$及以上}；\textbf{ROC/AUC曲线靠近图的左上角，曲线下面积达到$\boldsymbol{0.90}$}。此外，还将其与单一模型进行多维度对比，\textbf{准确率最高可提升$\boldsymbol{3\,\%}$}，模型提升效果良好。

		{\heiti 针对问题三}，需要利用上述已建立的模型，对未知数据集图像进行分类，并保存结果。经过读取模型、读取数据、数据集预处理、预测得出模型\textbf{在4942张图像中，识别出坑洼道路为3765张，识别为正常道路为1177张}。
		
		{\heiti 最后}，对所建立的模型的优缺点进行了中肯的评价、提出了模型的改进措施以及对模型进行了一定推广。
	\subsection{各问逐一分析}
	\begin{itemize}
		\item \textbf{问题一}：核心目的在于\textbf{建立更精准的识别道路坑洼边缘特性的模型，用于完成道路避障、道路修复等工作}。因此考虑对图像文件进行分割，有效地提取坑洼的边缘。首先，需要对图像文件进行预处理，包括人为再分类、文件夹分类，对标记错误的图像进行修正，并剔除异常的图像、原数据集保留与拓充、测试集数据预读取。之后从机理层面对道路的坑洼处进行分析，包括原图、热力图、RGB直方图、灰度直方图的对比、边缘及轮廓检测、以及阈值分割，从而有效地从机理层面分析特性。在此基础上建立基于YOLOv8s的segment图像分割及YOLOv8n的detect图像检测模型，对道路的坑洼特性进行学习、训练。在此之前需要对图像进行标注，标注文件的格式转换，训练集、测试集、验证集三者的划分，对训练集进行数据增强，配置训练文件。待模型训练完成后，再读取迭代过程中最优模型对测试集数据的坑洼边缘进行提取。
		\item \textbf{问题二}：核心目的在于\textbf{依据上述识别的坑洼边缘，估算坑洼面积占比}。因此考虑结合上述建立的检测与分割模型，得到其边缘像素坐标，估算其像素面积，进而计算坑洼面积占比。这里需要考虑到预测的坑洼的多样性，可能存在多个坑洼、坑洼边缘重叠等情况，因此需要对上述情况进行对应的处理，得到其坑洼面积，从而求得坑洼面积占比。
	\end{itemize}
	\subsection{行文思路}
	为了便于理清思路，绘制出本文解决问题的流程图，如\textcolor{blue}{\cref{fig:流程图}}所示。
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.34]{流程图-复赛.pdf}
		\caption{流程图}
		\label{fig:流程图}
	\end{figure}
	\section{模型的假设}
	\begin{itemize}
		\item \textbf{假设一}：所给数据集存在因某些原因而标注异常的图像数据的现象；
		\item \textbf{假设二}：所给数据集未能完整地表现出分类的特征。
	\end{itemize}
	\section{符号说明}
	\begin{center}
		\scalebox{0.9}{
		\begin{tabularx}{0.7\textwidth}{c@{\hspace{1pc}}|@{\hspace{2pc}}X}
			\Xhline{0.08em}
			\textbf{符号} & \multicolumn{1}{c}{\textbf{符号说明}}\\
			\Xhline{0.05em}
			$X$ & 图像横轴像素点的像素值\\
			$Y$ & 图像纵轴像素点的像素值\\
			$I_f$ & 滤波后的图像像素值\\
			$I$ & 原始图像像素值\\
			$G_s$ & 归一化权重\\
			$\Omega$ & 滤波邻域窗口\\
			$F_s$ & 空间域核函数\\
			$F_f$ & 像素值域核函数\\
			$E\left(x\right)$ & 数据均值\\
			$\lambda_n$ & 协方差矩阵特征值\\
			$\boldsymbol{u}$ & 协方差矩阵特征向量\\
			$C$ & SVM惩罚系数\\
			$S$ & 池化区域面积\\
			$N_{\text{TruePredict}}$ & 预测正确样本数\\
			$N_{\text{Predict}}$ & 预测样本总数\\
			\Xhline{0.08em}
		\end{tabularx}}
	\end{center}

	\textbf{注：}这里并未列出其余变量，这是由于它们在不同小节处有不同的含义，同时该表中也未列出专有定义的变量，这些变量在使用时会在相应位置进行详细说明。

	\section{模型的建立与求解}
	\subsection{初赛研究相关结论、坑洼特征分析}\label{分析}
	在初赛中，为了方便模型的分析、建立与求解，因此需要对图像数据进行机理分析，将标记为正常及坑洼的道路进行比较分析，并提取图像的特征。从以下几个方面进行分析：
	\begin{itemize}
		\item \textbf{原图、热力图对比}：分析坑洼与正常道路的表层区别。
		\item \textbf{RGB、灰度直方图}：分析坑洼与正常道路的图像信息分布。
		\item \textbf{边缘、轮廓检测}：分析坑洼与正常道路的边缘、轮廓特征。
		\item \textbf{阈值分割}：分析坑洼与正常道路的阈值分割特征。
	\end{itemize}
	
	但由于图片较多，故我们选择“normal133.jpg”及“potholes1.jpg”进行对比分析。以下是具体分析内容，该部分具体处理代码见\textbf{附录-C.11，Comparative Analysis of Normal and Potholes [正常与坑洼道路的比较分析]}。
	\subsubsection{原图、热力图对比}
	“normal133.jpg”及“potholes1.jpg”原图及热力图对比，如\textcolor{blue}{\cref{fig:原图对比}}及\textcolor{blue}{\cref{fig:热力图对比}}所示\textcolor{blue}{\footnote{本文所有图示、表格均已交叉引用，读者阅读PDF时可点击对应图表，进行跳转。}}。
	\begin{figure}[H]
		\centering
			\centering
			\includegraphics[scale=0.96]{[原图对比]normal133与potholes1.pdf}
			\caption{原图对比}
			\label{fig:原图对比}
	\end{figure}
	
	对比正常道路和坑洼道路的热力图，我们发现若整个道路的状况相同，没有坑洼，则那一片区域的热力值近似相同，热力图呈现的颜色相仿；若道路中的某一块状况与周围不同，如出现坑洼，则出现坑洼的区域热力值发生变化，使在热力图中呈现的颜色与周围存在差异。正常道路热力值基本一致，其热力图呈现色彩差别小，而坑洼道路中非坑洼处与坑洼处热力值相差较大，坑洼处与非坑洼处颜色相比存在较大差异。
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.5]{normal133热力图.pdf}
		\hspace{0.3in}
		\includegraphics[scale=0.5]{potholes1热力图.pdf}
		\caption{热力图对比}
		\label{fig:热力图对比}
	\end{figure}

	\subsubsection{RGB、灰度直方图}
	“normal133.jpg”及“potholes1.jpg”RGB及灰度直方图对比，如\textcolor{blue}{\cref{fig:RGB对比}}及\textcolor{blue}{\cref{fig:灰度对比}}所示。
	
	观察RGB直方图的对比，我们可以发现两者RGB三色整体变化具有一致性，但正常道路的RGB数值整体高于坑洼道路，且正常道路RGB峰值出现在像素值为$50\text{\textasciitilde}100$之间，而坑洼道路的RGB峰值出现在像素值为$0$与$250$处。

	观察灰度直方图的对比，我们可先发现灰度直方图中正常道路所呈现的峰值远高于坑洼道路，其中坑洼道路的峰值出现在图像两侧，而正常道路的峰值出现在图像的中心位置。此外正常道路数值波动明显大于坑洼道路。故我们认为灰度直方图所展现的数值大小、分布及波动情况可以较好体现道路的坑洼情况。
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.5]{normal133RGB直方图.pdf}
		\hspace{0.3in}
		\includegraphics[scale=0.5]{potholes1RGB直方图.pdf}
		\caption{RGB直方图对比}
		\label{fig:RGB对比}
	\end{figure}
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.5]{normal133灰度直方图.pdf}
		\hspace{0.3in}
		\includegraphics[scale=0.5]{potholes1灰度直方图.pdf}
		\caption{灰度直方图对比}
		\label{fig:灰度对比}
	\end{figure}
	
	\subsubsection{边缘、轮廓检测}
	“normal133.jpg”及“potholes1.jpg”边缘、轮廓检测对比，如\textcolor{blue}{\cref{fig:边缘检测}}及\textcolor{blue}{\cref{fig:轮廓检测}}所示。
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.5]{normal133边缘检测.pdf}
		\hspace{0.3in}
		\includegraphics[scale=0.45]{potholes1边缘检测.pdf}
		\caption{边缘检测}
		\label{fig:边缘检测}
	\end{figure}
	观察边缘检测的对比，我们可以发现边缘检测后正常道路色彩连续完整，而坑洼道路的坑洼部分在边缘检测后整体呈现深度黑色，与平整的砂石路面形成了强烈对比。故我们认为采用边缘检测可以较好判断道路状况。

	对道路的图片进行轮廓检测后，我们可以发现检测后无坑洼道路的图片颜色相同，而对于坑洼道路，图片上有很明显的颜色差别。经过分析，我们认为不同的颜色代表了道路的平整与坑洼，进行轮廓检测后，可以通过道路颜色的异同判断道路是否坑洼。
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.58]{normal133轮廓检测.pdf}
		\hspace{0.3in}
		\includegraphics[scale=0.5]{potholes1轮廓检测.pdf}
		\caption{轮廓检测}
		\label{fig:轮廓检测}
	\end{figure}

	\subsubsection{阈值分割}
	“normal133.jpg”及“potholes1.jpg”阈值分割对比，如\textcolor{blue}{\cref{fig:阈值分割}}所示。
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.5]{normal133阈值分割.pdf}
		\hspace{0.3in}
		\includegraphics[scale=0.42]{potholes1阈值分割.pdf}
		\caption{阈值分割}
		\label{fig:阈值分割}
	\end{figure}
	阈值分割将图片分为两类区域，对比正常道路与坑洼道路的阈值分割图，正常道路由于整个道路的状态相同，因此经阈值分割后，全部被划分为同一类，故颜色相同；而对于坑洼道路，道路中出现了与周围状态不同的区域，经过阈值分割后，呈现出不同的颜色，黑色为正常的道路，白色为道路中存在的坑洼，色彩对比强烈。

	\subsection{图像文件预处理}\label{图像文件预处理}
	该题原数据集为“data.zip”，包含301张图片，均为“jpg”格式，且在文件名中进行了标注。其中，文件名中包含“normal”字符的表示正常道路，共266张，包含“potholes”字符的为坑洼道路，共35张。考虑到数据的标注严谨性，首先对其进行人为再分类，将标注异常的进行重新标注或剔除。此外，为方便后续模型的分析、建立与求解，我们将“normal”与“potholes”图像文件置于“DATA”文件下的“normal”及“potholes”文件夹中。

	考虑到图像文件较少，用于训练的特征不足，因此对原数据集进行拓充，方法为从网络上获取坑洼道路的图像。

	此外，还需要预先读取测试集图像，以防破损文件影响预测过程。
	\subsubsection{人为再分类}
	考虑到数据的标注严谨性，我们首先对其进行人为再分类，将标注异常的进行重新标注或剔除。对于所给数据集，我们筛选出了以下几张图片，其标注存在问题，需要进行重新标注或剔除，如\textcolor{blue}{\cref{fig:异常图片}}所示，图中所标注的为该图像的原始文件名。
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.8]{异常图片.pdf}
		\caption{标注异常图片}
		\label{fig:异常图片}
	\end{figure}

	对于上述“(e) normal298.jpg”文件，我们选择剔除，其余划分为“potholes”类。具体处理见\textcolor{blue}{\cref{tab:异常图片处理}}。表中“potholes”代表将该问价视为坑洼道路，而“删除”代表将该文件删除，不参与后续模型的分析、建立、训练等。

\begin{table}[H]
	\centering
	\caption{异常标注图片的处理}
	\scalebox{0.9}{
	  \begin{tabular}{ccc}
	  \toprule
	  \textbf{编号}    & \textbf{原文件名}  & \textbf{处理方式} \\
	  \midrule
	  a     & normal9.jpg & potholes \\
	  b     & normal106.jpg & potholes \\
	  c     & normal173.jpg & potholes \\
	  d     & normal240.jpg & potholes \\
	  e     & normal298.jpg & 删除 \\
	  f     & normal312.jpg & potholes \\
	  g     & normal346.jpg & potholes \\
	  \bottomrule
	  \end{tabular}}
	\label{tab:异常图片处理}
\end{table}

  
	\subsubsection{文件夹分类}
	为了后续模型的分析、建立与求解，我们将“normal”与“potholes”图像文件置于“DATA”文件下的“normal”及“potholes”文件夹中，作为二分类的数据集。该处理我们使用Python的os及shutil库进行处理，首先指定根目录为“DATA”，再在其下创建“normal”与“potholes”文件夹，之后读取“DATA”文件夹下的所有文件，若文件名中含有“normal”字符，则将其放置于“normal”文件夹中，否则放置于“potholes”文件夹中。具体处理代码见\textbf{附录-C.10，Data Preprocessing [数据预处理]}。
	
	这里，我们可以大致观测数据集图像，如\textcolor{blue}{\cref{fig:图像数据观测}}所示。
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.5]{图像数据观测.pdf}
		\caption{图像数据观测}
		\label{fig:图像数据观测}
	\end{figure}
	至此，我们已将所有数据进行了人为再分类，并将数据集分别放于“normal”及“potholes”文件夹中，方便后续的处理。
	
	\subsubsection{原数据集的保留与拓充}
	由于在复赛中，本题任务需要重点分析坑洼道路的特性，因此，这里舍弃“normal”类图像，保留“potholes”图像，共计41张。

	考虑到保留的图像文件较少，学习特征不足，将会影响模型精度。同时，为了加强我们分析的可信度，我们从以下网站收集了坑洼图像，对“potholes”类图像数据进行拓充，来源见\textcolor{blue}{\cref{tab:数据来源}}所示。\textcolor{blue}{\footnote{\textbf{注：}拓充的图为原数据集中尚未出现过的图像，即原数据集为新数据集的子集。}}
	\begin{table}[htbp]
		\centering
		\caption{拓充数据来源}
		\scalebox{0.9}{
		  \begin{tabular}{cc}
		  \toprule
		  \textbf{数据源} & \textbf{网址} \\
		  \midrule
		  Kaggle & \url{https://www.kaggle.com/} \\
		  Github & \url{https://github.com/} \\
		  百度图片  & \url{https://image.baidu.com/} \\
		  \bottomrule
		  \end{tabular}}
		\label{tab:数据来源}
	  \end{table}
	\subsubsection{测试集数据预读取}
	此外，还需要预先读取测试集图像，以防破损文件影响预测过程。这里我们使用ultralytics库，并加上try-except异常处理，将数据加载至内存中，方便后续进行预测。
	
	经过读取，发现存在一个文件读取失败，为“guy7iodk.jpg”。因此，对于该张图片，我们对其进行单一读取、处理。而其余的4941张图正常读取，在后续可正常预测。
	\subsection{图像数据预处理}
	\subsubsection{opencv读取图像文件}
	考虑到图像文件较多，为方便后续处理，我们利用Python的opencv及os库，将数据加载至内存中。并分析各类中包含的图像数据个数，如\textcolor{blue}{\cref{tab:数据信息}}所示。这里罗列了原数据集与经过\textcolor{blue}{\nameref{图像文件预处理}}部分处理后的情况，同时还计算出正常道路与坑洼道路数据个数的比值。

\begin{table}[H]
	\centering
	\caption{数据信息}
	\scalebox{0.9}{
	  \begin{tabular}{ccccc}
	  \toprule
	  \textbf{类别} & \textbf{1：正常道路} & \textbf{0：坑洼道路} & \textbf{正常道路与坑洼道路图像数据个数比值} & \textbf{合计} \\
	  \midrule
	  原数据图像个数 & 266   & 35    & 7.60  & 301 \\
	  处理后图像个数 & 259   & 41    & 6.32  & 300 \\
	  \bottomrule
	  \end{tabular}}
	\label{tab:数据信息}
\end{table}
	
	\subsubsection{划分训练集与测试集}
	对于模型的学习，我们需要对数据集进行划分，划分为训练集与测试集。训练集用于模型的训练，测试集用于对模型性能的评估分析，检验模型效果。由于该数据集样本较少，我们设置训练集占比为$80\,\%$，测试集占比为$20\,\%$，且上述划分为采用Python的sklearn库中model\_selection.train\_test\_split方法及自定义地客观地随机划分。

	\subsubsection{训练集数据增强}
	观察数据集，发现其特征多数存在相似性；同时，数据集样本较少。因此，为避免模型的过拟合与欠拟合，这里我们需要对训练集进行数据增强\textcolor{blue}{\footnote{\textbf{注意：}这里数据增强是在划分训练集与测试集之后的，并且仅对训练集数据进行增强，并不对测试集进行划分。这是由于，若在划分训练集与测试集之前进行增强，则会造成数据泄露，影响模型的准确性，以及对未知数据的泛化能力。}}。从而增强模型对于未知数据集的泛化能力，提升模型的稳健性。

	这里，我们选用线性滤波中的\textbf{高斯滤波（Gauss Blur）}、非线性滤波中的\textbf{双边滤波（Bilateral Blur）}、以及对图像进行\textbf{顺时针旋转$\boldsymbol{\frac{\pi}{6}}$}处理。
	\begin{itemize}
		\item \textbf{高斯滤波}是一种线性平滑滤波。其基本原理是使用高斯核对图像进行卷积操作，进行加权平均的过程。每一个像素点的值，都由其邻域内的其他像素值和本身经过加权平均后得到。高斯核的标准差和大小决定了滤波器的效果，标准差越小，滤波器的效果越不明显，但是不会导致图像的细节信息丢失。其权值随着距离中心像素点的距离增加而逐渐减小，从而保留了图像的边缘信息。使用公式如下：
		\begin{equation}
			G\left(X,Y\right)=\frac{1}{2\pi\sigma^2}\exp\left(-\frac{X^2+Y^2}{2\sigma^2}\right)
		\end{equation}
		\item \textbf{双边滤波}是一种基于高斯滤波的非线性滤波方法，目的是解决高斯滤波造成的边缘模糊。结合图像的空间邻近度和像素值相似度，同时考虑空域信息和灰度相似性，实现保边去噪。双边滤波器比高斯滤波多一个高斯核。它是基于像素颜色分布的高斯滤波函数，所以在边缘附近，当两个像素距离很近时，只有同时当颜色很接近时影响才会较大，反之，虽然距离很近，但颜色差距较大，那么平滑权重也会很小。
		
		为实现双边滤波，我们首先定义滤波器的参数，即空间域核函数和像素值域核函数；然后计算出每个像素在空间域和像素值域上的权重；最后根据计算得到的权重，对每个像素的周围像素进行加权平均，以得到滤波后的像素值。其公式可表示为：
		\begin{equation}
			I_f\left( X,Y \right) =\frac{1}{G_s\left( X,Y \right)}\sum_{\left( i,j \right) \in \Omega}{I\left( X+i,Y+j \right) \cdot F_s\left( i,j \right)}\cdot F_f\left[ I\left( X,Y \right) ,I\left( X+i,Y+j \right) \right] 
		\end{equation}
		上式中各参量含义如下：
		\begin{itemize}
			\item $I_f\left(X,Y\right)$：滤波后的图像像素值；
			\item $I\left(X,Y\right)$：原始图像像素值；
			\item $G_s\left(X,Y\right)$：归一化权重；
			\item $\Omega$：滤波邻域窗口；
			\item $F_s\left(i,j\right)$：空间域核函数；
			\item $F_f\left[ I\left( X,Y \right) ,I\left( X+i,Y+j \right) \right]$：像素值域核函数。
		\end{itemize}
	\end{itemize}
\newpage
	这里，我们随机选择一张图片，将其经过上述处理后的图像一一展示，如\textcolor{blue}{\cref{fig:原图高斯双边滤波及旋转对比}}所示。经过上述处理后，在一定程度上可以增加训练集数据的多样性，从而模型可以学习到更多关于正常或坑洼道路的特征，因而有利于提升模型的泛化能力。
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.6]{原图高斯双边滤波及旋转对比.pdf}
		\caption{原图、高斯滤波、双边滤波、顺时针旋转30度}
		\label{fig:原图高斯双边滤波及旋转对比}
	\end{figure}

	经过上述滤波及旋转处理后，意味着将训练集增加3倍，即现有训练集为$681\times 0.8\times 4=2179$张图像。这里，我们将随机选择10张图像进行展示，如\textcolor{blue}{\cref{fig:数据增强}}所示。
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.6]{PCA-SVM训练样本.pdf}
		\caption{数据增强图像}
		\label{fig:数据增强}
	\end{figure}
	\subsection{坑洼特性提取模型的训练}
	\subsubsection{1}
	\subsection{测试集图像坑洼特性预测}

	\subsection{模型的评估}
	为更好地评估模型，我们针对上述三种模型进行多维度评估，并针对各模型进行针对性分析。这里我们用到的有：
	\begin{itemize}
		\item \textbf{模型训练及预测时间和（$T$）}
		\item \textbf{准确率（Accuracy）}：即预测正确数占总数的比例，其计算公式如下：
			\begin{equation}
				\mathrm{Accuracy}=\frac{N_{\mathrm{TruePredict}}}{N_{\mathrm{Predict}}} \label{Accuracy}
				\end{equation}
			其中，$N_{\mathrm{TruePredict}}$为预测正确的样本数，$N_{\mathrm{Predict}}$为被预测的样本总数；
		\item \textbf{损失值（Loss）}：这里我们采用Categorical Crossentropy\textcolor{blue}{\cite{损失函数}}，即交叉熵损失函数，其使用交叉熵（Cross Entropy）作为度量分类任务的差异，从而来衡量两者之间的概率分布的相似性，其公式如下：
			\begin{equation}
				\text{Loss}=-\sum_{i=1}^{\text{output size}}y_i\cdot\log\left(\hat{y}_i\right)
			\end{equation}
			其中，$y_i$为真实标签，$\hat{y}_i$为预测标签；
		\item \textbf{分类报告（Classification Report）}：其可以直观得到模型各项参数，包括每一类别的精确率（Precision），召回率（Recall），F1分数值（F1-Score）。对于这三项值，其计算公式如下：
		\begin{itemize}
			\item {\heiti 精确率}
			\begin{equation}
				\mathrm{Precision} = \frac{TP}{TP+FP} \label{Precision}
			\end{equation}
			\item {\heiti 召回率}
			\begin{equation}
				\mathrm{Recall} = \frac{TP}{TP+FN} \label{Recall}
			\end{equation}
			\item {\heiti F1分数值} 
			\begin{equation}
				\mathrm{F}1 = \frac{2\times \mathrm{Precision}\times \mathrm{Recall}}{\mathrm{Precision}+\mathrm{Recall}}=\frac{TP}{TP+\frac{1}{2}\left(FP+FN\right)} \label{F1-Score}
			\end{equation}
		\end{itemize}
		此外，对于模型的精确率、召回率，我们可以根据定义可以发现若这两项值较大，则模型效果较好。同时根据定义，我们可以发现模型的精确率、召回率在理想情况下是相差较小的，我们可以根据图示结果验证，符合预期效果。对于模型的F1分数值，其为精确率与召回率的调和平均数\textcolor{blue}{\cite{模型评测}}，因此当精确率与召回率均有较好表现时，F1分数值会有较优秀表现。我们也可对\textcolor{blue}{\eqref{F1-Score}}进行一定变换，可以得到：
		\begin{equation}
			\mathrm{F}1=\frac{2}{\dfrac{1}{\mathrm{Precision}}+\dfrac{1}{\mathrm{Recall}}} \label{ReacllNew}
		\end{equation}
		根据该式，我们可以得出上述结论。

		\item \textbf{混淆矩阵（Confusion Matrix）}：矩阵每一行表示样本标签的实际类别，在本题中表示道路类型：为正常还是坑洼的实际标签；每一列表示样本标签的预测类别，在本题中表示道路类型：为正常还是坑洼的预测标签。因此该图示的主对角线数据之和即为模型预测准确的样本数。这里此外还需要引入四项值，分别为$TP$、$FN$、$FP$、$TN$，其中T为True，F为False，这两个字母表示预测值与实际值是否相同；P为Positive，N为Negative，这两个字母表示预测出的是属于正类还是负类。而混淆矩阵可以直观地观察到预测准确与错误的情况，以及模型对于每一类别的区分程度。
		\item \textbf{特征曲线及曲线下面积曲线（Receiver Operating Characteristic/Area Under the Curve, ROC/AUC）}：首先我们需要引出模型的相关参数，定义如下：
		\begin{itemize}
			\item {\heiti 灵敏度}\textbf{（Sensitivity）}。灵敏度又被称为真阳性率，即$TP$率：
			\begin{equation}
				\mathrm{Sensitivity}=TPR=\frac{TP}{TP+FN} \label{Sensitivity}
			\end{equation}
			\item {\heiti 特异性}\textbf{（Specificity）}。特异性又被称为真阴性率，即$TN$率：
			\begin{equation}
				\mathrm{Specificity}=TNR=\frac{TN}{TN+FP} \label{Specificity}
			\end{equation}
			\item \textbf{1-Specificity}。称为假阳性率（False Positive Rate, $FPR$）：
			\begin{equation}
				FPR=1-\mathrm{Specificity}=\frac{FP}{FP+TN} \label{FPR}
			\end{equation}
			\item \textbf{1-Sensitivity}。称为假阴性率（False Negative Rate, $FNR$）：
			\begin{equation}
				FNR=1-\mathrm{Sensitivity}=\frac{FN}{FN+TP} \label{FNR}
			\end{equation}
		\end{itemize}
		$FPR$和$FNR$均对数据分布的变化不敏感\textcolor{blue}{\cite{Tharwat}}，因此这两个指标可以用于在不平衡的数据上建立的模型效果的评价。
		
		对于ROC/AUC曲线，其以每一类别的$1-\mathrm{Specificity}$即$FPR$为横坐标，以$\mathrm{Sensitivity}$即$TPR$为纵坐标，其可体现出模型的灵敏度与特异性之间的关系与差异。因此，该图的理想点位于左上角，即$FPR=0$且$TPR=1$。换言之，当曲线越靠近左上角，模型效果就越优。从而，我们可以得到另一项指标，即曲线下面积（Area Under the Curve, AUC），由上述分析可知，AUC值越高，模型的整体效果也就越优。
		\item \textbf{精确率-召回率曲线（Precision-Recall Curve）}：该图像可表现出分类的预测精度与召回率之间的关系\textcolor{blue}{\cite{Yellowbrick}}。对于预测精度与召回率的计算方式在“问题三”中已经叙述。因此图像的填充区域越大，分类效果越优。
		\item \textbf{交叉验证（Cross Validation）}：在该方法中，每一条数据用于训练的次数均相同，且恰好被检验一次。假设使用$k(k=2,3,4,\cdots)$折交叉验证，将数据分为大小相等的$k$份，在每一次学习时，选择其中一份作为检验集，而余下的数据作为训练集，上述过程重复$k$次。此外，检验集是互斥的，并且能够有效地覆盖数据集\textcolor{blue}{\cite{数据挖掘导论}}。
	\end{itemize}

	\subsubsection{三种模型的效果逐一分析}\label{模型效果}
	\begin{itemize}
		\item \textbf{PCA-SVM}

		通过查看Jupyter的代码执行记录：PCA模型在训练集训练耗时$50\,\text{s}+281\,\text{ms}$，在测试集适应耗时$2\,\text{s}+968\,\text{ms}$；SVM模型网格调优耗时$9\,\text{s}+948\,\text{ms}$，在训练集训练及在测试集测试共耗时$185\,\text{ms}$。通过在测试集的测试，经计算，该模型准确率为：$86.67\,\%$。此外，我们还得出其分类报告表，如\textcolor{blue}{\cref{tab:PCA-SVM分类报告表}}所示。
\begin{table}[htbp]
	\centering
	\caption{PCA-SVM分类报告表}
	\scalebox{0.9}{
	  \begin{tabular}{ccccc}
	  \toprule
	  \textbf{维度} & \textbf{precision} & \textbf{recall} & \textbf{f1-score} & \textbf{support} \\
	  \midrule
	  0：potholes & 0.50  & 0.25  & 0.33  & 8 \\
	  1：normal & 0.89  & 0.96  & 0.93  & 52 \\
	  accuracy &       &       & 0.87  & 60 \\
	  marco avg & 0.70  & 0.61  & 0.63  & 60 \\
	  weighted avg & 0.84  & 0.87  & 0.85  & 60 \\
	  \bottomrule
	  \end{tabular}}
	\label{tab:PCA-SVM分类报告表}
\end{table}
	观察上表，我们可以发现由于该模型尚未采用采样方法扩充数据集，虽然整体准确率较高，但对于各类别有明显的不平衡性。此外，该模型对于坑洼类别的精确率、召回率、F1分数值均较低，即说明该模型效果较为一般。
		\item \textbf{CNN}
		
		通过查看Jupyter的代码执行记录：CNN模型在训练集训练$50$轮耗时$5\,\text{m}+26\,\text{s}+888\,\text{ms}$，在测试集测试耗时$435\,\text{ms}$。通过在测试集的测试，经计算，该模型准确率为：$88.97\,\%$。此外，我们还得出其分类报告表，如\textcolor{blue}{\cref{tab:CNN分类报告表}}所示。
\begin{table}[htbp]
	\centering
	\caption{CNN分类报告表}
	\scalebox{0.9}{
	  \begin{tabular}{ccccc}
	  \toprule
	  \textbf{维度} & \textbf{precision} & \textbf{recall} & \textbf{f1-score} & \textbf{support} \\
	  \midrule
	  0：potholes & 0.90  & 0.88  & 0.89  & 68 \\
	  1：normal & 0.88  & 0.90  & 0.89  & 68 \\
	  accuracy &       &       & 0.89  & 136 \\
	  marco avg & 0.89  & 0.89  & 0.89  & 136 \\
	  weighted avg & 0.89  & 0.89  & 0.89  & 136 \\
	  \bottomrule
	  \end{tabular}}
	\label{tab:CNN分类报告表}
\end{table}

	观察该表，我们可以发现，该模型在新数据集上表现效果较优秀，各项指标均较为理想。此外，我们还绘制其在训练过程中在训练集及测试集上的准确率与损失值，如\textcolor{blue}{\cref{fig:CNN训练结果}}所示。通过观察该图，我们可以发现，随着迭代次数的增加，训练集上的准确率平稳而缓慢增加，测试集上的准确度经过起伏后也为上升趋势。而随着迭代次数增加，其训练集上的损失逐渐减少，而在测试集上其损失呈波动性，这可能是由于数据集过少，即缺乏代表性样本\textcolor{blue}{\cite{数据挖掘导论}}，易造成过拟合。

	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.65]{CNN训练结果.pdf}
		\caption{CNN训练结果}
		\label{fig:CNN训练结果}
	\end{figure}
		\item \textbf{CNN-SVM}
		
		CNN模型训练耗时$5\,\text{m}+26\,\text{s}+888\,\text{ms}$，提取图像特征耗时$482\,\text{ms}$；SVM分类器在所提取的特征上训练耗时$48\,\text{ms}$，在测试集上得出结果耗时$28\,\text{ms}$。通过在测试集的测试，经计算，该模型准确率为：$90\,\%$。该模型分类报告表如\textcolor{blue}{\cref{tab:CNN-SVM分类报告表}}所示。此外，为了方便查看，我们还绘制其分类报告图，如\textcolor{blue}{\cref{fig:CNN-SVM分类报告图}}所示。观察其分类报告，我们可以发现，该模型无论是对于正常或者是坑洼道路，其均能较好地识别，精度较高，并且召回率与F1分数值均较优秀。
\begin{table}[htbp]
	\centering
	\caption{CNN-SVM分类报告表}
	\scalebox{0.9}{
	  \begin{tabular}{ccccc}
	  \toprule
	  \textbf{维度} & \textbf{precision} & \textbf{recall} & \textbf{f1-score} & \textbf{support} \\
	  \midrule
	  0：potholes & 0.89  & 0.93  & 0.91  & 68 \\
	  1：normal & 0.92  & 0.88  & 0.90  & 68 \\
	  accuracy &       &       & 0.90  & 136 \\
	  marco avg & 0.91  & 0.90  & 0.90  & 136 \\
	  weighted avg & 0.91  & 0.90  & 0.90  & 136 \\
	  \bottomrule
	  \end{tabular}}
	\label{tab:CNN-SVM分类报告表}
  \end{table}

  	此外我们还根据模型在测试集上的预测结果与实际真实值进行比较，绘制出模型的混淆矩阵热力图，如\textcolor{blue}{\cref{fig:CNN-SVM混淆矩阵热力图}}所示。观察该图，我们可以发现：对于坑洼道路检测中，有$63$张图像的检测结果为坑洼，$5$张的检测结果为正常，即在$68$张图片为坑洼道路的图像中，识别正确的为$63$张，正确率达到$92.48\,\%$；对于正常道路的检测中，有$60$张图像的检测结果为正常，$8$张图像的检测结果为坑洼，即在$68$张为正常的道路图片中，识别正确的有$60$张，正确率为$88.24\,\%$，由此可见，此模型的效果较好。

\begin{figure}[H]
	\centering
	\begin{minipage}{0.48\linewidth}
		\centering
		\includegraphics[width=0.96\linewidth]{[CNN-SVC]分类报告.pdf}
		\caption{CNN-SVM分类报告图}
		\label{fig:CNN-SVM分类报告图}
	\end{minipage}
	%\qquad
	\begin{minipage}{0.48\linewidth}
		\centering
		\includegraphics[width=0.96\linewidth]{[CNN-SVC]混淆矩阵热力图.pdf}
		\caption{CNN-SVM混淆矩阵热力图}
		\label{fig:CNN-SVM混淆矩阵热力图}
	\end{minipage}
\end{figure}

同时，我们还绘制模型的ROC/AUC曲线以及精确率-召回率曲线，分别如\textcolor{blue}{\cref{fig:CNN-SVM ROC/AUC曲线}}与\textcolor{blue}{\cref{fig:CNN-SVM精确率召回率曲线}}所示。由图可知，该模型的ROC/AUC曲线主要分布在左上方，曲线下面积较大，即AUC值较高，为$0.90$；同时，该模型的精确率与召回率都位于较高的数值，说明该模型在对道路是否坑洼的识别上准确度高，模型效果较好。

\begin{figure}[H]
	\centering
	\begin{minipage}{0.48\linewidth}
		\centering
		\includegraphics[width=0.96\linewidth]{[CNN-SVC]ROC曲线.pdf}
		\caption{CNN-SVM\,ROC曲线}
		\label{fig:CNN-SVM ROC/AUC曲线}
	\end{minipage}
	%\qquad
	\begin{minipage}{0.48\linewidth}
		\centering
		\includegraphics[width=0.96\linewidth]{[CNN-SVC]精确率召回率曲线.pdf}
		\caption{CNN-SVM精确率-召回率曲线}
		\label{fig:CNN-SVM精确率召回率曲线}
	\end{minipage}
\end{figure}
	\end{itemize}

	此外，我们还对该模型进行了五折交叉验证，具体结果为：
	\begin{equation}
		\boldsymbol{[0.96428571, 0.88888889, 0.88888889,0.85185185, 0.92592593]}\nonumber
	\end{equation}
	因而，可以得到该模型的准确率在置信度为$95\,\%$的水平下，置信区间为$\left[82\,\%, 98\,\%\right]$。
	\subsubsection{模型对比}
	未选择最优模型，我们依据\textcolor{blue}{\nameref{模型效果}}部分，将各模型指标整理至一张表格中，如\textcolor{blue}{\cref{tab:模型效果对比}}所示。同时结合\textcolor{blue}{\cref{fig:CNN-SVM分类报告图}}\textasciitilde\textcolor{blue}{\cref{fig:CNN-SVM精确率召回率曲线}}，我们可以发现，在三者中，CNN-SVM模型的效果最优，其各项指标均较优秀。

\begin{table}[htbp]
	\centering
	\caption{模型效果对比}
	\scalebox{0.9}{
	  \begin{tabular}{ccccc}
	  \toprule
	  \textbf{指标} & \textbf{计算维度} & \textbf{PCA-SVM} & \textbf{CNN} & \textbf{CNN-SVM} \\
	  \midrule
	  \multirow{4}[2]{*}{\textbf{precision}} & 0：potholes & 0.50  & \textbf{0.90 } & 0.89  \\
			& 1：normal & 0.89  & 0.88  & \textbf{0.92 } \\
			& macro & 0.70  & 0.89  & \textbf{0.91 } \\
			& weighted & 0.84  & 0.89  & \textbf{0.91 } \\
	  \midrule
	  \multirow{4}[2]{*}{\textbf{recall}} & 0：potholes & 0.25  & 0.88  & \textbf{0.93 } \\
			& 1：normal & \textbf{0.96 } & 0.90  & 0.88  \\
			& macro & 0.61  & 0.89  & \textbf{0.90 } \\
			& weighted & 0.87  & 0.89  & \textbf{0.90 } \\
	  \midrule
	  \multirow{4}[2]{*}{\textbf{f1-score}} & 0：potholes & 0.33  & 0.89  & \textbf{0.91 } \\
			& 1：normal & \textbf{0.93 } & 0.89  & 0.90  \\
			& macro & 0.63  & 0.89  & \textbf{0.90 } \\
			& weighted & 0.85  & 0.89  & \textbf{0.90 } \\
	  \midrule
	  \textbf{accuracy} & 整体    & 0.87  & 0.89  & \textbf{0.90 } \\
	  \midrule
	  $\boldsymbol{T}$     & 共计    & 6\,m 8\,s 197\,ms & \textbf{5\,m 27\,s 323\,ms} & 6\,m 15\,s 37\,ms \\
	  \bottomrule
	  \end{tabular}}
	\label{tab:模型效果对比}
\end{table}

	\subsection{未知数据集的预测}
	对于该问题，具体处理代码见\textbf{附录-C.7，Predict [预测]}。我们按以下步骤进行处理：
	\begin{itemize}
		\item \textbf{读取已建立、训练完成的CNN-SVM模型}：将上述CNN-SVM模型利用keras及pickle库读取；
		\item \textbf{读取未知数据集}：将未知数据集的图像数据及其文件名（包括后缀名）一并读入，方便后续结果的输出，共计4942张图像；
		\item \textbf{对未知数据集进行预处理}：为了准确预测，需要将该数据集格式转换为前期训练模型输入的格式，避免预测错误。
		\item \textbf{对未知数据集进行预测}：首先传入CNN卷积神经网络，对数据集的特征进行提取；之后将提取的特征数据集传入SVM支持向量机中，进行预测；
		\item \textbf{结果查看}：经过上述步骤，将文件名与预测结果一一对应转为表格数据，方便查看；通过预测，模型在4942张图像中，识别出坑洼道路为3765张，识别为正常道路为1177张。
		\item \textbf{保存结果}：确认无误后，将上述表格输出为“test\_result.csv”文件。
	\end{itemize}
	\section{模型的评价与推广}
	\subsection{模型的评价}
	\begin{itemize}
		\item \textbf{模型的优点}
		\begin{enumerate}
			\item 未完全套用深度学习及传统机器学习模型，而是将CNN与SVM结合，使得模型能够处理非线性且稀疏数据并且对于权重的处理更加准确；
			\item 建立不同的模型进行对比分析，使结果更加准确；
			\item 在图像文件预处理时，对数据进行了人为再分类，使数据更准确有效；
			\item 在图像数据预处理时，对训练集进行高斯、双边滤波与旋转增强，提升模型的泛化能力；
			\item 由于所给原数据集分类样本不平衡，我们使用了采样的方法，适当增加数据集，使得原数据集为新数据集的子集，且分类样本平衡化。
		\end{enumerate}
		\item \textbf{模型的缺点}
		\begin{enumerate}
			\item 在对训练集进行数据增强时，仅使用了三种增强方法，在对特征进行学习时可能会有所遗漏；
			\item 由于所给原数据集分类样本不平衡，我们使用了采样的方法，适当增加数据集，使得原数据集为新数据集的子集，且分类样本平衡化。然后，该方法成本较高，对于数据集较少的任务，难以推广使用。
		\end{enumerate}
		\item \textbf{模型的改进}
		\begin{enumerate}
			\item 由于所给原数据集分类样本不平衡，除本文方法外，还可在模型中使用可选度量法和代价敏感学习进行解决\textcolor{blue}{\cite{数据挖掘导论}}；
			\item 对于CNN神经网络，可选择其他更适合本题的架构使模型更加精确；
			\item 在进行预测时，可以结合优化算法，如贝叶斯调优进一步优化模型超参数，以提高预测精度。其主要方法为：利用已知的超参数$x$和模型结果$y$来拟合一个代理模型，再利用采集函数选择下一个最优$\left(x,y\right)$，并用此新的$\left(x,y\right)$来优化代理模型，重复上述过程，最终得到调优后的参数，其算法伪代码如\textcolor{blue}{Algorithm \ref{贝叶斯优化}}所示。
			\item 对于本文的SVM模型，我们可用其他分类效果更好的模型，如Stacking多模型融合集成学习。
		\end{enumerate}
	\end{itemize}

	\scalebox{0.85}{
	\begin{algorithm}[H]
		\label{贝叶斯优化}
		\KwIn{初始化点个数$n_0$，最大迭代次数$N$，代理模型$g\left(x\right)$，采集函数$\alpha\left(x|D\right)$}
		\KwOut{最优候选评估点：$\left\{x^{*},y^{*}\right\}$}
		\textbf{Step 1：}随机初始化$n_0$点$X_{\text{init}}=\left\{x_0,x_1,\cdots,x_{n_0-1}\right\}$\
		
		\textbf{Step 2：}获取其对应的函数值$f\left(X_\text{init}\right)$，初始点集$D_0=\left\{X_\text{init},f\left(X_\text{init}\right)\right\}$，令$t=n_0$，$D_{t-1}=D_0$\

		\While{t<N}{
			\textbf{Step 3：}根据当前获得的点集$D_{t-1}$，构建代理模型$g\left(x\right)$\
			
			\textbf{Step 4：}基于代理模型$g\left(x\right)$，最大化采集函数$\alpha\left(x|D_{t-1}\right)$，获得下一个评估点：$x_t=\text{argmin}\,\alpha\left(x|D_{t-1}\right)$\
			
			\textbf{Step 5：}获得评估点$x_t$的函数值$f\left(x_t\right)$，将其加入到当前评估点集合中：$D_t=D_{t-1}U\left\{x_t,f\left(x_t\right)\right\}$，转\textbf{Step 3}\
		}

		\KwResult{最优候选评估点：$\left\{x^{*},y^{*}\right\}$}
		\caption{贝叶斯优化框架}
	\end{algorithm}}
	\subsection{模型的推广}
	此模型具备处理具有复杂特征图像的能力，且能够较好地进行分类，因此除了判别道路坑洼与否外，我们也可将此其应用于重大基础设施建设前的勘探过程中，对工程建设地区的形进行判别，准确且快速地确定该地区是否适合建设该工程。以此类推，除了工程方面，此模型还可应用于交通预警，此模型适用于各地交管局及城市建设市政管理部门。在一定程度上改善了当前的预警机制，为减少民众财产损失做出了贡献。
	\newpage
	
	\phantomsection
	\addcontentsline{toc}{section}{\textbf{参考文献}}
	\begin{spacing}{1.08}
	\begin{thebibliography}{99}
	\bibitem{曹江华}曹江华. 复杂背景下非结构化道路可行驶区域检测研究[D].浙江科技学院,2021.

	\bibitem{刘建新}刘建新,史志仙.概率论与数理统计[M].北京:高等教育出版社,2016:115.

	\bibitem{汪海燕}汪海燕,黎建辉,杨风雷.支持向量机理论及算法研究综述[J].计算机应用研究,2014,31(05):1281-1286.

	\bibitem{张松兰}张松兰,王鹏,徐子伟.基于统计相关的缺失值数据处理研究[J].统计与决策,2016,No.456(12):13-16.

	\bibitem{李莎}李莎.基于改进卷积神经网络的上市企业财务风险预测研究[J].现代信息科技,2023,7(20):111-115.

	\bibitem{孟琪琳}孟琪琳,窦燕.基于EMD-CNN-LSTM模型的铁路客运量短期预测研究[J/OL].铁道运输与经济:1-9[2023-10-30].

	\bibitem{ZHANGP}ZHANG P,FANG Y.Research on text classification algorithm based on machine learning[J].Journal of Physics Conference Series,2020,1624:042010.

	\bibitem{李毅泉}李毅泉. 基于注意力机制的显著区域提取研究和实现[D].北京交通大学,2007.

	\bibitem{何铠}何铠,管有庆,龚锐.基于深度学习和支持向量机的文本分类模型[J].计算机技术与发展,2022,32(07):22-27.

	\bibitem{刘幸倩}刘幸倩. 基于注意力增强卷积神经网络的滚动轴承故障诊断方法研究[D].东北石油大学,2023.

	\bibitem{损失函数}CSDN.损失函数：categorical\_crossentropy[EB/OL].\url{https://blog.csdn.net/qq_40661327/article/details/107034575}.

	\bibitem{模型评测}知乎.模型评测:PRECISION、RECALL、F1-score[EB/OL].\url{https://zhuanlan.zhihu.com/p/519982682}.

	\bibitem{Tharwat}A.Tharwat, Applied Computing and Informatics (2018). \url{https://doi.org/10.1016/j.aci.2018.08.003}.

	\bibitem{Yellowbrick}Yellowbrick.Precision-Recall Curves - Yellowbrick v1.5 documentation[EB/OL].\url{https://www.scikit-yb.org/en/latest/api/classifier/prcurve.html}.

	\bibitem{数据挖掘导论}Tan P N, Steinbach M, Karpatne A, et al. Introduction to Data Mining[M]. Pearson, 2019:389.
	\end{thebibliography}
	\end{spacing}
	\newpage

	\phantomsection
	\addcontentsline{toc}{section}{\textbf{附\hspace{2pc}录}}

	% \appendix
	% \ctexset{section={format={\zihao{-4}\heiti\raggedright}}}
	\begin{center}
		\heiti\zihao{4} 附\hspace{2pc}录
	\end{center}

% \phantomsection
% \addcontentsline{toc}{subsection}{[A]图示}
% 	% \section*{[A]图表}
% 	\noindent{\heiti [A]图示}
% \newpage
% \phantomsection
% \addcontentsline{toc}{subsection}{[B]支撑文件列表}
% 	% \section*{[B]支撑文件列表}
	\noindent{\heiti [A]支撑文件列表}
	~\\

	支撑文件列表如下（列表中不包含原始数据集）：
	% Table generated by Excel2LaTeX from sheet 'Sheet2'
	\begin{table}[H]
		\centering
		\scalebox{0.9}{
		  \begin{tabular}{cc}
		  \toprule
		  \textbf{文件（夹）名} & \textbf{描述} \\
		  \midrule
		  html & 包括所有解决问题的源程序运行结果 \\
		  ipynb & 包括所有解决问题的源程序源代码 \\
		  models  & 已训练好的模型文件 \\
		  process & 包括所有模型训练、预测的过程结果 \\
		  py  & 包括所有解决问题的源程序输出python文件 \\
		  yaml & 包括所有模型训练的配置文件 \\
		  test\_result2.csv  & 坑洼面积占比估算结果 \\
		  \bottomrule
		  \end{tabular}}
	\end{table}
  
\newpage
% \phantomsection
% \addcontentsline{toc}{subsection}{[C]使用的软件、环境}
	% \section*{[C]使用的软件、环境}
	\noindent{\heiti [B]使用的软件、环境}
	% ~\\

	\textbf{B.1 使用的软件及版本}
	\begin{itemize}
		\item TeX Live 2022
		\item Visual Studio Code 1.85.0
		\item WPS Office 2023冬季更新（15990）
		\item Python 3.10.4 [MSC v.1929 64 bit (AMD64)] on win32
		\item Pycharm 2023.3 (Professional Edition)
	\end{itemize}
	
	\textbf{B.2 模型训练所用计算机配置}
	\begin{itemize}
		\item Intel(R) Core(TM) i5-10200H CPU @ 2.40GHz
		\item NVIDIA GeForce GTX 1650 Ti
		\item NVIDIA CUDA 11.7.102 driver
		\item 16.0 GB RAM
		\item Windows 10 家庭中文版 22H2
	\end{itemize}

	\textbf{Python环境下所用使用到的库及其版本}
\begin{table}[htbp]
	\centering
	\setlength{\aboverulesep}{0pt}
	\setlength{\belowrulesep}{0pt}
	\scalebox{0.9}{
	  \begin{tabular}{cc||cc}
	  \toprule
	  \textbf{库}     & \textbf{版本}    & \textbf{库}     & \textbf{版本} \\
	  \midrule
	  collections & 内置库   & jupyterlab-widgets & 3.0.9 \\
	  opencv-python & 4.8.1.78 & keras & 2.14.0 \\
	  h5py  & 3.10.0 & matplotlib & 3.8.0 \\
	  jupyter & 1.0.0 & numpy & 1.26.1 \\
	  jupyter\_client & 8.5.0 & os    & 内置库 \\
	  jupyter-console & 6.6.3 & pandas & 2.1.2 \\
	  jupyter\_core & 5.4.0 & pickle & 内置库 \\
	  jupyter-events & 0.8.0 & random & 内置库 \\
	  jupyter-lsp & 2.2.0 & shutil & 内置库 \\
	  jupyter\_server & 2.9.1 & sklearn & 1.3.2 \\
	  jupyter\_server\_terminals & 0.4.4 & tensorflow & 2.14.0 \\
	  jupyterlab & 4.0.7 & warnings & 内置库 \\
	  jupyterlab-pygments & 0.2.2 & yellowbrick & 1.50  \\
	  jupyterlab\_server & 2.25.0 &       &  \\
	  \bottomrule
	  \end{tabular}}
\end{table}

\newpage
% \phantomsection
% \addcontentsline{toc}{subsection}{[D]问题解决源程序}
	% \section*{[D]问题解决源程序}
\noindent{\heiti [C]问题解决源程序}

\textbf{C.1 ista2txt [ista转txt]}
\begin{python}
import json
import os
import shutil

category_mapping = {"potholes": 0}
# ISAT格式的实例分割标注文件
ISAT_FOLDER = "./annotations"
# YOLO格式的实例分割标注文件
YOLO_FOLDER = "./labels"

# 创建YoloV8标注的文件夹
if not os.path.exists(YOLO_FOLDER):
    os.makedirs(YOLO_FOLDER)


# 载入所有的ISAT的JSON文件
for filename in os.listdir(ISAT_FOLDER):
    if not filename.endswith(".json"):
        # 不是json格式, 跳过
        continue
    # 载入ISAT的JSON文件
    with open(os.path.join(ISAT_FOLDER, filename), "r") as f:
        isat = json.load(f)
    # 提取文件名(不带文件后缀)
    image_name = filename.split(".")[0]
    # Yolo格式的标注文件名, 后缀是txt
    yolo_filename = f"{image_name}.txt"
    # 写入信息
    with open(os.path.join(YOLO_FOLDER, yolo_filename), "w") as f:
        # 获取图像信息
        # - 图像宽度

        image_width = isat["info"]["width"]

        # try:
        #     image_width = isat["info"]["width"]
        # except KeyError:
        #     break
        # - 图像高度
        image_height = isat["info"]["height"]
        # 获取实例标注数据
        for annotation in isat["objects"]:
            # 获取类别名称
            category_name = annotation["category"]
            # 如果不在类别名称字典里面，跳过
            if category_name not in category_mapping:
                continue
            # 从字典里面查询类别ID
            category_id = category_mapping[category_name]
            # 提取分割信息
            segmentation = annotation["segmentation"]
            segmentation_yolo = []
            # 遍历所有的轮廓点
            for segment in segmentation:
                # 提取轮廓点的像素坐标 x, y
                x, y = segment
                # 归一化处理
                x_center = x/image_width
                y_center = y/image_height
                # 添加到segmentation_yolo里面
                segmentation_yolo.append(f"{x_center:.4f} {y_center:.4f}")
            segmentation_yolo_str = " ".join(segmentation_yolo)
            # 添加一行Yolo格式的实例分割数据
            # 格式如下: class_id x1 y1 x2 y2 ... xn yn\n
            f.write(f"{category_id} {segmentation_yolo_str}\n")
\end{python}

\textbf{C.2 xml2txt [xml转txt]}
\begin{python}
import xml.etree.ElementTree as ET
import os, cv2
import numpy as np
from os import listdir
from os.path import join

#将给定的XML格式的标注文件转换为YOLOv8所需的TXT格式，并记录数据集中的类别信息
classes = []

def convert(size, box):
    dw = 1. / (size[0])
    dh = 1. / (size[1])
    x = (box[0] + box[1]) / 2.0 - 1
    y = (box[2] + box[3]) / 2.0 - 1
    w = box[1] - box[0]
    h = box[3] - box[2]
    x = x * dw
    w = w * dw
    y = y * dh
    h = h * dh
    return (x, y, w, h)


def convert_annotation(xmlpath, xmlname):
    with open(xmlpath, "r", encoding='utf-8') as in_file:
        txtname = xmlname[:-4] + '.txt'
        txtfile = os.path.join(txtpath, txtname)
        tree = ET.parse(in_file)
        root = tree.getroot()
        filename = root.find('filename')
        img = cv2.imdecode(np.fromfile('{}/{}.{}'.format(imgpath, xmlname[:-4], postfix), np.uint8), cv2.IMREAD_COLOR)
        h, w = img.shape[:2]
        res = []
        for obj in root.iter('object'):
            cls = obj.find('name').text
            if cls not in classes:
                classes.append(cls)
            cls_id = classes.index(cls)
            xmlbox = obj.find('bndbox')
            b = (float(xmlbox.find('xmin').text), float(xmlbox.find('xmax').text), float(xmlbox.find('ymin').text),
                 float(xmlbox.find('ymax').text))
            bb = convert((w, h), b)
            res.append(str(cls_id) + " " + " ".join([str(a) for a in bb]))
        if len(res) != 0:
            with open(txtfile, 'w+') as f:
                f.write('\n'.join(res))


if __name__ == "__main__":
    postfix = 'jpg'
    imgpath = 'VOCdevkit/JPEGImages'
    xmlpath = 'VOCdevkit/Annotations'
    txtpath = 'VOCdevkit/txt'
    
    if not os.path.exists(txtpath):
        os.makedirs(txtpath, exist_ok=True)
    
    list = os.listdir(xmlpath)
    error_file_list = []
    for i in range(0, len(list)):
        try:
            path = os.path.join(xmlpath, list[i])
            if ('.xml' in path) or ('.XML' in path):
                convert_annotation(path, list[i])
                print(f'file {list[i]} convert success.')
            else:
                print(f'file {list[i]} is not xml format.')
        except Exception as e:
            print(f'file {list[i]} convert error.')
            print(f'error message:\n{e}')
            error_file_list.append(list[i])
    print(f'this file convert failure\n{error_file_list}')
    print(f'Dataset Classes:{classes}')
\end{python}

\textbf{C.3 Segment Spilt Data [图像分割数据集划分]}
\begin{python}
import os
import random
from tqdm import tqdm

# 指定 images 文件夹路径
image_dir = "./images"
# 指定 labels 文件夹路径
label_dir = "./labels"
# 创建一个空列表来存储有效图片的路径
valid_images = []
# 创建一个空列表来存储有效 label 的路径
valid_labels = []

# 遍历 images 文件夹下的所有图片
for image_name in os.listdir(image_dir):
    # 获取图片的完整路径
    image_path = os.path.join(image_dir, image_name)
    # 获取图片文件的扩展名
    ext = os.path.splitext(image_name)[-1]
    # 根据扩展名替换成对应的 label 文件名
    label_name = image_name.replace(ext, ".txt")
    # 获取对应 label 的完整路径
    label_path = os.path.join(label_dir, label_name)
    # 判断 label 是否存在
    if not os.path.exists(label_path):
        # 删除图片
        os.remove(image_path)
        print("deleted:", image_path)
    else:
        # 将图片路径添加到列表中
        valid_images.append(image_path)
        # 将label路径添加到列表中
        valid_labels.append(label_path)
        # print("valid:", image_path, label_path)
# 遍历每个有效图片路径
for i in tqdm(range(len(valid_images))):
    image_path = valid_images[i]
    label_path = valid_labels[i]
    # 随机生成一个概率
    r = random.random()
    # 判断图片应该移动到哪个文件夹
    # train：valid：test = 7:2:1
    if r < 0.1:
        # 移动到 test 文件夹
        destination = "./datasets/test"
    elif r < 0.3:
        # 移动到 valid 文件夹
        destination = "./datasets/valid"
    else:
        # 移动到 train 文件夹
        destination = "./datasets/train"
    # 生成目标文件夹中图片的新路径
    image_destination_path = os.path.join(destination, "images", os.path.basename(image_path))
    # 移动图片到目标文件夹
    os.rename(image_path, image_destination_path)
    # 生成目标文件夹中 label 的新路径
    label_destination_path = os.path.join(destination, "labels", os.path.basename(label_path))
    # 移动 label 到目标文件夹
    os.rename(label_path, label_destination_path)
print("valid images:", valid_images)
# 输出有效label路径列表
print("valid labels:", valid_labels)
\end{python}

\textbf{C.4 Detect Spilt Data [图像检测数据集划分]}
\begin{python}
import os, shutil
from sklearn.model_selection import train_test_split

# 用于将图像和标注文件按照一定比例划分为训练集、验证集和测试集，并将它们分别复制到相应的文件夹中
val_size = 0.1
test_size = 0.1
postfix = 'jpg'
imgpath = 'VOCdevkit/JPEGImages'
txtpath = 'VOCdevkit/txt'

os.makedirs('images/train', exist_ok=True)
os.makedirs('images/val', exist_ok=True)
os.makedirs('images/test', exist_ok=True)
os.makedirs('labels/train', exist_ok=True)
os.makedirs('labels/val', exist_ok=True)
os.makedirs('labels/test', exist_ok=True)

listdir = [i for i in os.listdir(txtpath) if 'txt' in i]
train, test = train_test_split(listdir, test_size=test_size, shuffle=True, random_state=0)
train, val = train_test_split(train, test_size=val_size, shuffle=True, random_state=0)
print(f'train set size:{len(train)} val set size:{len(val)} test set size:{len(test)}')

for i in train:
    shutil.copy('{}/{}.{}'.format(imgpath, i[:-4], postfix), 'images/train/{}.{}'.format(i[:-4], postfix))
    shutil.copy('{}/{}'.format(txtpath, i), 'labels/train/{}'.format(i))

for i in val:
    shutil.copy('{}/{}.{}'.format(imgpath, i[:-4], postfix), 'images/val/{}.{}'.format(i[:-4], postfix))
    shutil.copy('{}/{}'.format(txtpath, i), 'labels/val/{}'.format(i))

for i in test:
    shutil.copy('{}/{}.{}'.format(imgpath, i[:-4], postfix), 'images/test/{}.{}'.format(i[:-4], postfix))
    shutil.copy('{}/{}'.format(txtpath, i), 'labels/test/{}'.format(i))
\end{python}

\textbf{C.5 YOLOv8s Segment [YOLOv8s图像分割]}
\begin{python}
#!/usr/bin/env python
# coding: utf-8

# In[1]:


from ultralytics import YOLO

# In[3]:


model = YOLO('yolov8s-seg.pt')
results = model.train(data="dataset/VOCdevkit/datasets/potholes.yaml", epochs=300, device='cpu')

# In[2]:


model = YOLO('potholesSegment.pt')

# In[3]:


resultOne = model('potholes\\part1', save=True)

# In[4]:


resultTwo = model('potholes\\part2', save=True)

# In[5]:


resultThree = model('potholes\\part3', save=True)

# In[3]:


resultFour = model('potholes\\part4', save=True)

# In[4]:


resultFive = model('potholes\\part5', save=True)

# In[3]:


resultSix = model('potholes\\part6', save=True)

# In[4]:


resultSeven = model('potholes\\part7', save=True)

# In[3]:


resultEight = model('potholes\\part8', save=True)
\end{python}

\textbf{C.6 YOLOv8n Detect [YOLOv8n图像检测]}
\begin{python}
#!/usr/bin/env python
# coding: utf-8

# In[1]:


from ultralytics import YOLO


# In[47]:


model = YOLO("potholesDetect.pt")
resultsOne = model.predict(source="potholes\\partOne", save=True, save_conf=True, save_txt=True,imgsz=416)


# In[48]:


model = YOLO("potholesDetect.pt")
resultsTwo = model.predict(source="potholes\\partTwo", save=True, save_conf=True, save_txt=True,imgsz=416)
\end{python}

\textbf{C.7 Coordinate Transformation [坐标转换]}
\begin{python}
#!/usr/bin/env python
# coding: utf-8

# In[1]:


import os
import cv2
import torch
import numpy as np

# In[2]:


label_path = 'potholes\labels'
image_path = 'potholes\images'


def xywhn2xyxy(x, w=416, h=416, padw=0, padh=0):
    y = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)
    y[:, 0] = w * (x[:, 0] - x[:, 2] / 2) + padw  # top left x
    y[:, 1] = h * (x[:, 1] - x[:, 3] / 2) + padh  # top left y
    y[:, 2] = w * (x[:, 0] + x[:, 2] / 2) + padw  # bottom right x
    y[:, 3] = h * (x[:, 1] + x[:, 3] / 2) + padh  # bottom right y
    return y


folder = os.path.exists('potholes\\labelsNew')
if not folder:
    os.makedirs('potholes\\labelsNew')

folderlist = os.listdir(label_path)
for i in folderlist:
    label_path_new = os.path.join(label_path, i)
    with open(label_path_new, 'r') as f:
        lb = np.array([x.split() for x in f.read().strip().splitlines()], dtype=np.float32)  # labels
        print(lb)
    read_label = label_path_new.replace(".txt", ".jpg")
    read_label_path = read_label.replace(label_path, image_path)
    print(read_label_path, "ddd")
    img = cv2.imread(str(read_label_path))
    h, w = img.shape[:2]
    lb[:, 1:] = xywhn2xyxy(lb[:, 1:], w, h, 0, 0)

    for _, x in enumerate(lb):
        class_label = int(x[0])

        with open('potholes\\labelsNew\\' + i, 'a') as fw:
            fw.write(str(int(x[0])) + ' ' + str(x[5]) + ' ' + str(x[1]) + ' ' + str(x[2]) + ' ' + str(x[3]) + ' ' + str(
                x[4]) + '\n')
\end{python}

\textbf{C.8 Proportional Calculation [面积占比估算]}
\begin{python}
#!/usr/bin/env python
# coding: utf-8

# In[1]:


import os
import cv2
import numpy as np
import pandas as pd

# In[2]:


labelsList = os.listdir('potholes\\labelsNew')
imagesList = os.listdir('potholes\\images')

# In[3]:


labelsDf = pd.DataFrame(labelsList, columns=['fnames'])
labelsDf

# In[4]:


Npotholes = pd.read_csv('potholes\\detectResults.txt', sep=' ', header=None)
Npotholes = Npotholes.drop([0, 1, 3, 5, 6], axis=1)
Npotholes.columns = ['fnames', 'Npotholes']
Npotholes['fnames'] = Npotholes['fnames'].str.split('\\').str[-1]
Npotholes['Npotholes'] = Npotholes['Npotholes'].str.replace('(', '')
Npotholes['fnames'] = Npotholes['fnames'].str.replace(':', '')
Npotholes

# In[5]:


Npotholes['Npotholes'] = Npotholes['Npotholes'].str.replace('no', '0')
Npotholes['Npotholes'] = Npotholes['Npotholes'].astype('int')
Npotholes

# In[6]:


data = pd.DataFrame(columns=['Type', '置信度', 'x1', 'y1', 'x2', 'y2', 'fnames'])
data

# In[7]:


for i in labelsList:
    with open('potholes\\labelsNew\\' + i, 'r') as f:
        content = f.readlines()
        lb = np.array([x.strip().split() for x in content], dtype=np.float32)  # labels
        lb = pd.DataFrame(lb, columns=['Type', '置信度', 'x1', 'y1', 'x2', 'y2'])
        lb['fnames'] = i

    data = data._append(lb, ignore_index=True)

data

# In[8]:


data = data.drop('Type', axis=1)
data = data[['fnames', '置信度', 'x1', 'y1', 'x2', 'y2']]
data['fnames'] = data['fnames'].str.replace('.txt', '.jpg')
data

# In[9]:


data = pd.merge(data, Npotholes, on='fnames', how='right')
data

# In[10]:


potholesImages = pd.DataFrame(imagesList, columns=['fnames'])
potholesImages

# In[11]:


for i in imagesList:
    img = cv2.imread('potholes\\images\\' + i)
    height, width, channels = img.shape
    potholesImages.loc[potholesImages['fnames'] == i, 'height'] = height
    potholesImages.loc[potholesImages['fnames'] == i, 'width'] = width

potholesImages

# In[12]:


data = pd.merge(data, potholesImages, on='fnames', how='left')
data

# In[13]:


data['S'] = data['height'] * data['width']
data['X'] = data['x2'] - data['x1']
data['Y'] = data['y2'] - data['y1']
data['Spotholes'] = data['X'] * data['Y']
data['Spotholes/S'] = data['Spotholes'] / data['S'] * 100
data

# In[14]:


data['Spotholes/S'] = data['Spotholes/S'].fillna(data['Spotholes/S'].mean())
data['Spotholes/S'] = data['Spotholes/S'].apply(np.ceil)
data['Spotholes/S'] = data['Spotholes/S'].astype('int')
data

# In[15]:


dataNew = data.groupby('fnames').apply(lambda x: x.loc[x['Spotholes/S'].idxmax()])
dataNew

# In[16]:


dataNew = dataNew.reset_index(drop=True)
dataNew

# In[17]:


dataR = pd.read_csv('未知数据集预测效果.csv')
dataR = dataR.drop(['imgType', 'label', 'isTrue'], axis=1)
dataR

# In[18]:


Results = pd.merge(dataR, dataNew, on='fnames', how='left')
Results

# In[19]:


# 修改guy7iodk.jpg行，height=1587，width=1200，S=1587*1200=1904400，Spotholes=954792，Spotholes/S=50
Results.loc[Results['fnames'] == 'guy7iodk.jpg', 'height'] = 1587
Results.loc[Results['fnames'] == 'guy7iodk.jpg', 'width'] = 1200
Results.loc[Results['fnames'] == 'guy7iodk.jpg', 'S'] = 1904400
Results.loc[Results['fnames'] == 'guy7iodk.jpg', 'Spotholes'] = 954792
Results.loc[Results['fnames'] == 'guy7iodk.jpg', 'Spotholes/S'] = 50
Results

# In[20]:


Results.loc[Results['imgClass'] == 'normal', 'Spotholes/S'] = 0
Results

# In[21]:


Results['Spotholes/S'] = Results['Spotholes/S'].astype('int')
Results

# In[22]:


Results.to_csv('SpotholesS.csv', index=False)
\end{python}

\textbf{C.9 Detect Segment Analyze [图像检测与图像分割效果分析]}
\begin{python}
#!/usr/bin/env python
# coding: utf-8

# In[1]:


import pandas as pd

dataDetect = pd.read_csv('detectResults.txt', sep=' ', header=None)
dataSegment = pd.read_csv('segmentResults.txt', sep=' ', header=None)

# In[2]:


dataDetect = dataDetect.drop([0, 1, 3, 5, 6], axis=1)
dataDetect.columns = ['fnames', 'Detect-Npotholes']
dataDetect['fnames'] = dataDetect['fnames'].str.split('\\').str[-1]
dataDetect['Detect-Npotholes'] = dataDetect['Detect-Npotholes'].str.replace('(', '')
dataDetect['fnames'] = dataDetect['fnames'].str.replace(':', '')
dataDetect['Detect-Npotholes'] = dataDetect['Detect-Npotholes'].str.replace('no', '0')
dataDetect['Detect-Npotholes'] = dataDetect['Detect-Npotholes'].astype('int')
dataDetect

# In[3]:


dataSegment = dataSegment.drop([0, 1, 3, 5, 6], axis=1)
dataSegment.columns = ['fnames', 'Segment-Npotholes']
dataSegment['fnames'] = dataSegment['fnames'].str.split('\\').str[-1]
dataSegment['Segment-Npotholes'] = dataSegment['Segment-Npotholes'].str.replace('(', '')
dataSegment['fnames'] = dataSegment['fnames'].str.replace(':', '')
dataSegment['Segment-Npotholes'] = dataSegment['Segment-Npotholes'].str.replace('no', '0')
dataSegment['Segment-Npotholes'] = dataSegment['Segment-Npotholes'].astype('int')
dataSegment

# In[4]:


data = pd.merge(dataDetect, dataSegment, on='fnames', how='left')
data

# In[5]:


# DetectFalse为Detect-Npotholes为0的个数
# DetectTrue为Detect-Npotholes不为0的个数
# SegmentFalse为Segment-Npotholes为0的个数
# SegmentTrue为Segment-Npotholes不为0的个数
DetectFalse = 0
DetectTrue = 0
SegmentFalse = 0
SegmentTrue = 0
for i in range(len(data)):
    if data['Detect-Npotholes'][i] == 0:
        DetectFalse += 1
    else:
        DetectTrue += 1
    if data['Segment-Npotholes'][i] == 0:
        SegmentFalse += 1
    else:
        SegmentTrue += 1

# In[6]:


DetectTrue / (DetectFalse + DetectTrue)

# In[7]:


SegmentTrue / (SegmentFalse + SegmentTrue)
\end{python}

\textbf{C.10 Data Preprocessing [数据预处理]}
\begin{python}
#!/usr/bin/env python
# coding: utf-8

# In[1]:


import os
import shutil

# 指定目录"DATA"
path = "DATA"

# 在"DATA"文件夹中创建"normal"和"potholes"文件夹
os.mkdir(os.path.join(path, "normal"))
os.mkdir(os.path.join(path, "potholes"))

# 读取"DATA"文件夹，若文件名中含有"normal"，则将其放置于"normal"文件夹中，否则放置于"potholes"文件夹中
files = os.listdir(path)
for file in files:
    if "normal" in file:
        shutil.move(os.path.join(path, file), os.path.join(path, "normal"))
    else:
        shutil.move(os.path.join(path, file), os.path.join(path, "potholes"))

\end{python}

\textbf{C.11 Comparative Analysis of Normal and Potholes [正常与坑洼道路比较分析]}
\begin{python}
#!/usr/bin/env python
# coding: utf-8

# In[1]:


import cv2
import numpy as np
import matplotlib.pyplot as plt

# In[2]:


normalImg = cv2.imread('DATA\\normal\\normal133.jpg')
potholesImg = cv2.imread('DATA\\potholes\\potholes1.jpg')

# In[3]:


plt.rcParams['font.sans-serif'] = ['Times New Roman']
plt.rcParams['axes.unicode_minus'] = False


# In[4]:


def cv_show(img):
    b, g, r = cv2.split(img)
    img = cv2.merge([r, g, b])
    plt.imshow(img)


# In[5]:


cv_show(normalImg)

# In[6]:


cv_show(potholesImg)

# In[7]:


color = ('b', 'g', 'r')

for i, col in enumerate(color):
    histr = cv2.calcHist([normalImg], [i], None, [256], [0, 256])
    plt.plot(histr, color=col)

plt.legend(['Blue', 'Green', 'Red'])
plt.xlim([0, 256])
plt.xticks(fontsize=10)
plt.yticks(fontsize=10)
plt.title('(a) normal133.jpg', y=-0.2, fontsize=12)
plt.xlabel('Pixel Value', fontsize=11)
plt.ylabel('Number of Pixels', fontsize=11)
plt.savefig('Figures\\normal133RGB直方图.pdf', bbox_inches='tight')

# In[8]:


color = ('b', 'g', 'r')

for i, col in enumerate(color):
    histr = cv2.calcHist([potholesImg], [i], None, [256], [0, 256])
    plt.plot(histr, color=col)

plt.legend(['Blue', 'Green', 'Red'])
plt.xlim([0, 256])
plt.xticks(fontsize=10)
plt.yticks(fontsize=10)
plt.title('(b) potholes1.jpg', y=-0.2, fontsize=12)
plt.xlabel('Pixel Value', fontsize=11)
plt.ylabel('Number of Pixels', fontsize=11)
plt.savefig('Figures\\potholes1RGB直方图.pdf', bbox_inches='tight')

# In[9]:


plt.style.use('ggplot')
plt.hist(normalImg.ravel(), 256, [0, 256], color='grey')
plt.title('(a) normal133.jpg', y=-0.2, fontsize=12)
plt.xlabel('Pixel Value', fontsize=11)
plt.ylabel('Number of Pixels', fontsize=11)
plt.savefig('Figures\\normal133灰度直方图.pdf', bbox_inches='tight')

# In[10]:


plt.style.use('ggplot')
plt.hist(potholesImg.ravel(), 256, [0, 256], color='grey')
plt.title('(b) potholes1.jpg', y=-0.2, fontsize=12)
plt.xlabel('Pixel Value', fontsize=11)
plt.ylabel('Number of Pixels', fontsize=11)
plt.savefig('Figures\\potholes1灰度直方图.pdf', bbox_inches='tight')

# In[11]:


# 边缘检测
gray = cv2.cvtColor(normalImg, cv2.COLOR_BGR2GRAY)
edges = cv2.Canny(gray, 100, 200)
plt.imshow(edges, cmap='gray')
plt.title('(a) normal133.jpg', y=-0.2, fontsize=12)
plt.savefig('Figures\\normal133边缘检测.pdf', bbox_inches='tight')

# In[12]:


# 边缘检测
gray = cv2.cvtColor(potholesImg, cv2.COLOR_BGR2GRAY)
edges = cv2.Canny(gray, 100, 200)
plt.imshow(edges, cmap='gray')
plt.title('(b) potholes1.jpg', y=-0.2, fontsize=12)
plt.savefig('Figures\\potholes1边缘检测.pdf', bbox_inches='tight')

# In[13]:


plt.imshow(normalImg)
plt.colorbar()
plt.title('(a) normal133.jpg', y=-0.3, fontsize=12)
plt.savefig('Figures\\normal133热力图.pdf')

# In[14]:


plt.imshow(potholesImg)
plt.colorbar()
plt.title('(b) potholes1.jpg', y=-0.3, fontsize=12)
plt.savefig('Figures\\potholes1热力图.pdf')

# In[15]:


# 阈值分割
hsv = cv2.cvtColor(normalImg, cv2.COLOR_BGR2HSV)
lower_blue = np.array([90, 50, 50])
upper_blue = np.array([130, 255, 255])
mask = cv2.inRange(hsv, lower_blue, upper_blue)
plt.imshow(mask, cmap='gray')
plt.title('(a) normal133.jpg', y=-0.2, fontsize=12)
plt.savefig('Figures\\normal133阈值分割.pdf', bbox_inches='tight')

# In[16]:


# 阈值分割
hsv = cv2.cvtColor(potholesImg, cv2.COLOR_BGR2HSV)
lower_blue = np.array([90, 50, 50])
upper_blue = np.array([130, 255, 255])
mask = cv2.inRange(hsv, lower_blue, upper_blue)
plt.imshow(mask, cmap='gray')
plt.title('(b) potholes1.jpg', y=-0.2, fontsize=12)
plt.savefig('Figures\\potholes1阈值分割.pdf', bbox_inches='tight')

# In[17]:


# 转换为灰度图像
gray = cv2.cvtColor(normalImg, cv2.COLOR_BGR2GRAY)
# 二值化
ret, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)
# 轮廓检测
contours, hierarchy = cv2.findContours(binary, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)
# 绘制轮廓
cv2.drawContours(normalImg, contours, -1, (0, 0, 255), 3)

plt.subplot(1, 2, 1)
plt.title('(a) Original', y=-0.4, fontsize=12)
plt.imshow(normalImg)
plt.subplot(1, 2, 2)
plt.imshow(binary, cmap='gray')
plt.title('(b) Contours', y=-0.4, fontsize=12)
plt.savefig('Figures\\normal133轮廓检测.pdf', bbox_inches='tight')

# In[18]:


# 转换为灰度图像
gray = cv2.cvtColor(potholesImg, cv2.COLOR_BGR2GRAY)
# 二值化
ret, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)
# 轮廓检测
contours, hierarchy = cv2.findContours(binary, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)
# 绘制轮廓
cv2.drawContours(potholesImg, contours, -1, (0, 0, 255), 3)

plt.subplot(1, 2, 1)
plt.title('(a) Original', y=-0.4, fontsize=12)
plt.imshow(potholesImg)
plt.subplot(1, 2, 2)
plt.imshow(binary, cmap='gray')
plt.title('(b) Contours', y=-0.4, fontsize=12)
plt.savefig('Figures\\potholes1轮廓检测.pdf', bbox_inches='tight')
\end{python}

\textbf{C.12 Images Display [图像展示]}
\begin{python}
#!/usr/bin/env python
# coding: utf-8

# In[1]:


import cv2

img = cv2.imread("DATA\\potholes\\potholes1.jpg", cv2.IMREAD_COLOR)
img = cv2.resize(img, [256, 256])

Gaussian = cv2.GaussianBlur(img, (3, 3), 1)
Bilateral = cv2.bilateralFilter(img, 9, 75, 75)
Rotate = cv2.warpAffine(img, cv2.getRotationMatrix2D((img.shape[1] / 2, img.shape[0] / 2), 30, 1),
                        (img.shape[1], img.shape[0]))

cv2.imwrite("potholes1_Gaussian.jpg", Gaussian)
cv2.imwrite("potholes1_Bilateral.jpg", Bilateral)
cv2.imwrite("potholes1_Rotate.jpg", Rotate)

\end{python}

\newpage
% \phantomsection
% \addcontentsline{toc}{subsection}{[D]问题解决源程序}
	% \section*{[D]问题解决源程序}
\noindent{\heiti [D]模型训练配置文件}

\textbf{D.1 segment.yaml}
\begin{python}
# Train/val/test sets as 1) dir: path/to/imgs, 2) file: path/to/imgs.txt, or 3) list: [path/to/imgs1, path/to/imgs2, ..]
# path: ../datasets  # dataset root dir
train: E:/University/PythonProject/UltralyticsPotholes/dataset/VOCdevkit/datasets/train/images  # train images (relative to 'path') 128 images
val: E:/University/PythonProject/UltralyticsPotholes/dataset/VOCdevkit/datasets/valid/images  # val images (relative to 'path') 128 images
test: E:/University/PythonProject/UltralyticsPotholes/dataset/VOCdevkit/datasets/test/images # test images (optional)

# Classes
names:
  0: potholes
\end{python}

\textbf{D.2 detect.yaml}
\begin{python}
# dataset path
train: E:/University/PythonProject/UltralyticsMaster/dataset/images/train
val: E:/University/PythonProject/UltralyticsMaster/dataset/images/val
test: E:/University/PythonProject/UltralyticsMaster/dataset/images/test

# number of classes
nc: 1

# class names
names: ['potholes']
\end{python}

\textbf{D.3 YOLOv8-seg.yaml}
\begin{python}
# Parameters
nc: 1  # number of classes
scales: # model compound scaling constants, i.e. 'model=yolov8n-seg.yaml' will call yolov8-seg.yaml with scale 'n'
  # [depth, width, max_channels]
  n: [0.33, 0.25, 1024]
  s: [0.33, 0.50, 1024]
  m: [0.67, 0.75, 768]
  l: [1.00, 1.00, 512]
  x: [1.00, 1.25, 512]

# YOLOv8.0n backbone
backbone:
  # [from, repeats, module, args]
  - [-1, 1, Conv, [64, 3, 2]]  # 0-P1/2
  - [-1, 1, Conv, [128, 3, 2]]  # 1-P2/4
  - [-1, 3, C2f, [128, True]]
  - [-1, 1, Conv, [256, 3, 2]]  # 3-P3/8
  - [-1, 6, C2f, [256, True]]
  - [-1, 1, Conv, [512, 3, 2]]  # 5-P4/16
  - [-1, 6, C2f, [512, True]]
  - [-1, 1, Conv, [1024, 3, 2]]  # 7-P5/32
  - [-1, 3, C2f, [1024, True]]
  - [-1, 1, SPPF, [1024, 5]]  # 9

# YOLOv8.0n head
head:
  - [-1, 1, nn.Upsample, [None, 2, 'nearest']]
  - [[-1, 6], 1, Concat, [1]]  # cat backbone P4
  - [-1, 3, C2f, [512]]  # 12

  - [-1, 1, nn.Upsample, [None, 2, 'nearest']]
  - [[-1, 4], 1, Concat, [1]]  # cat backbone P3
  - [-1, 3, C2f, [256]]  # 15 (P3/8-small)

  - [-1, 1, Conv, [256, 3, 2]]
  - [[-1, 12], 1, Concat, [1]]  # cat head P4
  - [-1, 3, C2f, [512]]  # 18 (P4/16-medium)

  - [-1, 1, Conv, [512, 3, 2]]
  - [[-1, 9], 1, Concat, [1]]  # cat head P5
  - [-1, 3, C2f, [1024]]  # 21 (P5/32-large)

  - [[15, 18, 21], 1, Segment, [nc, 32, 256]]  # Segment(P3, P4, P5)
\end{python}
\end{document}